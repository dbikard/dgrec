{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e8d418-f8f9-4a6c-966c-3e3fde2c185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp DGRec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6db12-bc09-43a2-8a4c-525f01a9094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d643b7-10db-4bd7-90a7-230d4ad71468",
   "metadata": {},
   "source": [
    "# DGRec_model\n",
    "\n",
    "> A module to generate and estimate likelihoods of TR to VR generation using a Long-Short Term Memory (LSTM) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ed92f5-a877-4f73-ad7c-ba810901791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "import os\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#from tqdm import tqdm\n",
    "#from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d1f1f5a-5f96-4f83-bebb-bd66b88cac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from pathlib import Path\n",
    "\n",
    "#data_path = Path(\"../../dgrec/example_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c6d3436-5ba2-49a7-b270-8fad51def4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# One-hot encoding function for DNA sequences (A, C, G, T)\n",
    "def one_hot_encode(sequence, vocab_size=4):\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    integer_encoded = [mapping[base] for base in sequence]\n",
    "    onehot_encoded = to_categorical(integer_encoded, num_classes=vocab_size)\n",
    "    return onehot_encoded\n",
    "\n",
    "def one_hot_decode(encoded_sequence):\n",
    "    # Reverse the mapping used in encoding\n",
    "    reverse_mapping = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    \n",
    "    # Get the index of the max value in each one-hot vector (which corresponds to the base index)\n",
    "    integer_decoded = np.argmax(encoded_sequence, axis=1)\n",
    "    \n",
    "    # Convert the integer sequence back to the DNA bases using the reverse mapping\n",
    "    decoded_sequence = ''.join([reverse_mapping[i] for i in integer_decoded])\n",
    "    \n",
    "    return decoded_sequence\n",
    "\n",
    "def separate_model(model): \n",
    "    # model: the lstm model of two layers used for predicting TR to VR\n",
    "    # The present function is for disentangling the two layers to improve the generation speed of VR given TR\n",
    "    weights = model.get_weights()\n",
    "    \n",
    "    # Define DNA alphabet size and model parameters\n",
    "    vocab_size = 4  # e.g., A, T, C, G\n",
    "    lstm_units = 8  # Number of LSTM units\n",
    "    \n",
    "    \n",
    "    # Input layers (for the original model)\n",
    "    inputs = layers.Input(shape=(None, vocab_size))\n",
    "    inputs_mut = layers.Input(shape=(None, 2 * vocab_size + 1))  # Mutation input\n",
    "    \n",
    "    # Initialize initial states (h and c)\n",
    "    initial_h = layers.Input(shape=(lstm_units,))  # Hidden state\n",
    "    initial_c = layers.Input(shape=(lstm_units,))  # Cell state\n",
    "    \n",
    "    # First Bidirectional LSTM layer (Separate from second LSTM)\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(inputs)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(x)\n",
    "    \n",
    "    # Extract the weights for the Bidirectional LSTM and TimeDistributed Dense Layer\n",
    "    bidirectional_lstm_weights = model.layers[1].get_weights()  # Assuming first LSTM layer is at index 1\n",
    "    time_distributed_dense_weights = model.layers[3].get_weights()  # Assuming TimeDistributed dense is at index 3\n",
    "    \n",
    "    outputs2 = layers.Input(shape=(None, vocab_size))\n",
    "    # Second LSTM layer with return_state=True, passing initial states\n",
    "    inputs2 = layers.Concatenate()([inputs, inputs_mut, outputs2])  # Concatenate inputs\n",
    "    lstm_outputs, state_h, state_c = layers.LSTM(\n",
    "        lstm_units, return_sequences=True, return_state=True\n",
    "    )(inputs2, initial_state=[initial_h, initial_c])\n",
    "    \n",
    "    # Extract the weights for the second LSTM layer\n",
    "    second_lstm_weights = model.layers[5].get_weights()  # Assuming second LSTM layer is at index 5\n",
    "    \n",
    "    # Final output layer\n",
    "    final_outputs = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(lstm_outputs)\n",
    "    \n",
    "    # Extract the weights for the final Dense layer\n",
    "    final_dense_weights = model.layers[6].get_weights()  # Assuming final dense layer is at index 6\n",
    "    \n",
    "    # Rebuild the model with the separated layers\n",
    "    # First, build the part for Bidirectional LSTM and TimeDistributed Dense Layer\n",
    "    first_part_model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    first_part_model.set_weights([bidirectional_lstm_weights[0], bidirectional_lstm_weights[1], bidirectional_lstm_weights[2],  bidirectional_lstm_weights[3],  bidirectional_lstm_weights[4],  bidirectional_lstm_weights[5],\n",
    "                                  time_distributed_dense_weights[0], time_distributed_dense_weights[1]])\n",
    "    \n",
    "    # Now, build the second part (second LSTM layer and final output layer)\n",
    "    second_part_model = models.Model([inputs, inputs_mut,outputs2, initial_h, initial_c], [final_outputs, state_h, state_c])\n",
    "    second_part_model.set_weights([second_lstm_weights[0], second_lstm_weights[1], second_lstm_weights[2],\n",
    "                                   final_dense_weights[0], final_dense_weights[1]])\n",
    "    return first_part_model,second_part_model\n",
    "    \n",
    "# Function to generate a sequence\n",
    "def generate_sequence_from_onehot(X, firstmodel,secondmodel):\n",
    "    \"\"\"\n",
    "    Generate a sequence iteratively using one cell of the second LSTM.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: np.array, Input sequences (one-hot encoded, reversed).\n",
    "    - model: Trained Keras model for generation.\n",
    "    - vocab_size: int, Size of the vocabulary (e.g., 4 for A, T, C, G).\n",
    "    \n",
    "    Returns:\n",
    "    - generated_sequence: np.array, Generated sequence (one-hot encoded).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract batch size and input sequence dimensions\n",
    "    size = X.shape[0]  # Batch size\n",
    "    seq_length = X.shape[1]  # Input sequence length\n",
    "    vocab_size = X.shape[2]  # Input sequence length\n",
    "    lstm_units=8\n",
    "    # Initialize mutation input and output sequence arrays\n",
    "    X_mut = np.zeros((size, 1, vocab_size * 2 + 1))  # Mutation input\n",
    "    generated_sequence = np.zeros((size, seq_length, vocab_size))  # Output sequence array\n",
    "    \n",
    "    # Initialize the mutation input (start token for first step)\n",
    "    X_mut[:, 0, -1] = 1  # Special start token in mutation input\n",
    "    \n",
    "    # Initial LSTM state\n",
    "    initial_h = np.zeros((size, lstm_units))  # Hidden state\n",
    "    initial_c = np.zeros((size, lstm_units))  # Cell state\n",
    "\n",
    "    # Initialize the first LSTM block output (Bidirectional LSTM)\n",
    "    first_layer_output = np.array(firstmodel.predict(X))\n",
    "     # Use the last output from the first LSTM block to start generation\n",
    "    # Iterative sequence generation\n",
    "    for t in tqdm(range(seq_length), desc=\"Generating sequence\"):\n",
    "        # Concatenate the inputs for the second LSTM layer\n",
    "        # Get the output, hidden state (h), and cell state (c) from the second LSTM\n",
    "        output, new_h, new_c = secondmodel([X[:, t:t+1, :], X_mut, first_layer_output[:, t:t + 1, :],initial_h,initial_c])\n",
    "        # Sample the next nucleotide (one-hot vector) based on the predicted probabilities\n",
    "        next_word_idx = np.array([np.random.choice(vocab_size, p=dist) for dist in np.array(output)[:, 0, :]])\n",
    "        next_word_one_hot = np.zeros((size, vocab_size))\n",
    "        next_word_one_hot[np.arange(size), next_word_idx] = 1\n",
    "        next_word_one_hot = np.expand_dims(next_word_one_hot, axis=1)  # Shape: (batch_size, 1, vocab_size)\n",
    "\n",
    "        # Store the generated nucleotide in the sequence\n",
    "        generated_sequence[:, t:t+1, :] = next_word_one_hot\n",
    "\n",
    "        # Update the initial states for the next timestep\n",
    "        initial_h, initial_c = np.array(new_h), np.array(new_c)\n",
    "\n",
    "        # Update the mutation input for the next timestep\n",
    "        if t < seq_length - 1:\n",
    "            X_mut[:, 0, vocab_size:2*vocab_size] = next_word_one_hot[:, 0, :]  # Use generated nucleotide as input mutation\n",
    "            X_mut[:, 0, :vocab_size] = X[:, t, :]  # Use original sequence as mutation input\n",
    "            X_mut[:, 0, -1] = 0\n",
    "    return generated_sequence\n",
    "\n",
    "def sequences_same_length(sequences):\n",
    "    \"\"\"\n",
    "    Check if all sequences in a list have the same length.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences: list of sequences (e.g., strings, lists, or arrays)\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if all sequences have the same length, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the length of the first sequence\n",
    "    first_length = len(sequences[0])\n",
    "\n",
    "    # Check if all sequences have the same length\n",
    "    for seq in sequences:\n",
    "        if len(seq) != first_length:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "#model_TR_to_VR = tf.keras.models.load_model(data_path/'LSTM_model.keras') #the LSTM model\n",
    "#firstmodel,secondmodel=separate_model(model_TR_to_VR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cd3e5fc-6aa2-49c2-93f9-8d1e2819b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generate_sequences(X_seq): #X _seq is a list of sequences ATCG sequences (faster if same length)\n",
    "    \"\"\"\n",
    "    Generate list of VR from list of TR (one TR-> one VR).\n",
    "    Parameters:\n",
    "    - X_seq: list of TR sequences (e.g., strings, lists, or arrays)\n",
    "    Returns:\n",
    "    - list: list of VR sequence strings given TR sequences (corresponding to the initial TR list).\n",
    "    \"\"\"\n",
    "    X=np.array([one_hot_encode(x[::-1]) for x in X_seq])\n",
    "    if sequences_same_length(X_seq): # check that all sequences have same length: allow for fast generation\n",
    "        Y=generate_sequence_from_onehot(X,firstmodel,secondmodel)\n",
    "    else:\n",
    "        Y=[generate_sequence_from_onehot(np.array([x]),firstmodel,secondmodel)[0] for x in X]\n",
    "    return [one_hot_decode(y)[::-1] for y in Y] # return sequence in good order\n",
    "\n",
    "def generate_sequences_oneTR(TR,n=1000):\n",
    "    \"\"\"\n",
    "    Generate list of VR from one TR (one TR-> n VR).\n",
    "    Parameters:\n",
    "    - TR: one TR sequence (e.g., strings, lists, or arrays)\n",
    "    -n: integer corresponding to the number of VR to generate\n",
    "    Returns:\n",
    "    - list: list of n VR sequence strings given the one TR sequence.\n",
    "    \"\"\"\n",
    "    TR_list=[TR]*n\n",
    "    return generate_sequences(TR_list)\n",
    "    \n",
    "def to_tensor_inputs(*args):\n",
    "    return [tf.convert_to_tensor(x) for x in args]\n",
    "    \n",
    "def compute_likelihood(TR, VR):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood of VR from TR.\n",
    "\n",
    "    TR, VR: same-length strings\n",
    "    Returns: list of log-likelihoods\n",
    "    \"\"\"\n",
    "    assert len(TR) == len(VR), \"Mismatched lengths\"\n",
    "\n",
    "    vocab_size = 4\n",
    "    lstm_units = 8\n",
    "    seq_length = len(TR)\n",
    "\n",
    "    # Encode and reverse sequences\n",
    "    X = np.array([one_hot_encode(TR[::-1])])\n",
    "    Y = np.array([one_hot_encode(VR[::-1])])\n",
    "\n",
    "    # First model output\n",
    "    first_output = firstmodel.predict(X, verbose=0)\n",
    "\n",
    "    # Build mutation input (start with all start tokens)\n",
    "    X_mut = np.zeros((1, seq_length, 2 * vocab_size + 1))\n",
    "    X_mut[:, 0, -1] = 1  # Start tokens\n",
    "\n",
    "    # Fill mutation input from known inputs\n",
    "    for t in range(1, seq_length):\n",
    "        X_mut[:, t, :vocab_size] = X[:, t - 1, :]\n",
    "        X_mut[:, t, vocab_size:2*vocab_size] = Y[:, t - 1, :]\n",
    "\n",
    "    # Initial LSTM states\n",
    "    initial_h = np.zeros((batch_size, lstm_units))\n",
    "    initial_c = np.zeros((batch_size, lstm_units))\n",
    "\n",
    "    # Run full sequence in one call\n",
    "    outputs, _, _ = secondmodel.predict([X, X_mut, first_output, initial_h, initial_c], verbose=0)\n",
    "\n",
    "    # Compute log-likelihoods\n",
    "    log_likelihoods = []\n",
    "    for b in range(1):\n",
    "        log_prob = 0.\n",
    "        for t in range(seq_length):\n",
    "            prob = outputs[b, t, np.argmax(Y[b, t])]\n",
    "            log_prob += np.log(prob + 1e-8)\n",
    "        log_likelihoods.append(log_prob)\n",
    "\n",
    "    return log_likelihoods[0]\n",
    "\n",
    "def compute_likelihood_batch(TR_batch, VR_batch):\n",
    "    \"\"\"\n",
    "    Compute log-likelihoods of generating each VR[i] from TR[i] (batched).\n",
    "\n",
    "    TR_batch, VR_batch: list of same-length strings\n",
    "    Returns: list of log-likelihoods\n",
    "    \"\"\"\n",
    "    assert all(len(tr) == len(vr) for tr, vr in zip(TR_batch, VR_batch)), \"Mismatched lengths\"\n",
    "\n",
    "    vocab_size = 4\n",
    "    lstm_units = 8\n",
    "    batch_size = len(TR_batch)\n",
    "    seq_length = len(TR_batch[0])\n",
    "\n",
    "    # Encode and reverse sequences\n",
    "    X = np.array([one_hot_encode(tr[::-1]) for tr in TR_batch])\n",
    "    Y = np.array([one_hot_encode(vr[::-1]) for vr in VR_batch])\n",
    "\n",
    "    # First model output\n",
    "    first_output = firstmodel.predict(X, verbose=0)\n",
    "\n",
    "    # Build mutation input (start with all start tokens)\n",
    "    X_mut = np.zeros((batch_size, seq_length, 2 * vocab_size + 1))\n",
    "    X_mut[:, 0, -1] = 1  # Start tokens\n",
    "\n",
    "    # Fill mutation input from known inputs\n",
    "    for t in range(1, seq_length):\n",
    "        X_mut[:, t, :vocab_size] = X[:, t - 1, :]\n",
    "        X_mut[:, t, vocab_size:2*vocab_size] = Y[:, t - 1, :]\n",
    "\n",
    "    # Initial LSTM states\n",
    "    initial_h = np.zeros((batch_size, lstm_units))\n",
    "    initial_c = np.zeros((batch_size, lstm_units))\n",
    "\n",
    "    # Run full sequence in one call\n",
    "    outputs, _, _ = secondmodel.predict([X, X_mut, first_output, initial_h, initial_c], verbose=0)\n",
    "\n",
    "    # Compute log-likelihoods\n",
    "    log_likelihoods = []\n",
    "    for b in range(batch_size):\n",
    "        log_prob = 0.\n",
    "        for t in range(seq_length):\n",
    "            prob = outputs[b, t, np.argmax(Y[b, t])]\n",
    "            log_prob += np.log(prob + 1e-8)\n",
    "        log_likelihoods.append(log_prob)\n",
    "\n",
    "    return log_likelihoods\n",
    "\n",
    "def compute_likelihood_matrix(TR_list, VR_list, batch_size=64):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood matrix of generating each VR from each TR using batching.\n",
    "    \"\"\"\n",
    "    result_matrix = []\n",
    "\n",
    "    for i in range(0, len(TR_list)):\n",
    "        row = []\n",
    "        TR = TR_list[i]\n",
    "        valid_VRs = [vr for vr in VR_list if len(vr) == len(TR)]\n",
    "\n",
    "        if not valid_VRs:\n",
    "            row = [-np.inf] * len(VR_list)\n",
    "        else:\n",
    "            batch_TRs = [TR] * len(valid_VRs)\n",
    "            log_likelihoods = compute_likelihood_batch(batch_TRs, valid_VRs, firstmodel, secondmodel)\n",
    "            idx = 0\n",
    "            for vr in VR_list:\n",
    "                if len(vr) != len(TR):\n",
    "                    row.append(-np.inf)\n",
    "                else:\n",
    "                    row.append(log_likelihoods[idx])\n",
    "                    idx += 1\n",
    "\n",
    "        result_matrix.append(row)\n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e858453-b103-4b01-8815-5f3eac97a455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
